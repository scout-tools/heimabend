{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "import collections\n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tag_category = pd.read_parquet('data/tag_category.parquet')\n",
    "df_tag = pd.read_parquet('data/tag.parquet')\n",
    "\n",
    "df_event = pd.read_parquet('data/event.parquet')\n",
    "df_event_tags = pd.read_parquet('data/event_tags.parquet')\n",
    "df_event_material_list = pd.read_parquet('data/event_material_list.parquet')\n",
    "\n",
    "df_experiment = pd.read_parquet('data/experiment.parquet')\n",
    "df_experiment_item = pd.read_parquet('data/experiment_item.parquet')\n",
    "\n",
    "df_request_log = pd.read_parquet('data/request_log.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first Experiment\n",
    "df_experiment_item = df_experiment_item[df_experiment_item['created_at'] > '2021-03-10']\n",
    "df_experiment_item = df_experiment_item[df_experiment_item['created_at'] < '2021-03-27']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_apperances_of_tag(html, tag):\n",
    "    soup = BS(html)\n",
    "    return len(soup.find_all(tag))\n",
    "\n",
    "def get_text(html):\n",
    "    soup = BS(html)\n",
    "    whitelist = [\n",
    "      'p',\n",
    "      'li',\n",
    "      'span'\n",
    "    ]\n",
    "    text_elements = [t for t in soup.find_all(text=True) if t.parent.name in whitelist]\n",
    "    return ' '.join(text_elements)\n",
    "  \n",
    "def count_char(string, char):\n",
    "    return string.count(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html features\n",
    "df_features = df_event\n",
    "\n",
    "df_features['p_tag'] = df_features['description'].apply(num_apperances_of_tag, args=('p',))\n",
    "df_features['img_tag'] = df_features['description'].apply(num_apperances_of_tag, args=('img',))\n",
    "df_features['b_tag'] = df_features['description'].apply(num_apperances_of_tag, args=('b',))\n",
    "df_features['li_tag'] = df_features['description'].apply(num_apperances_of_tag, args=('li',))\n",
    "df_features['span_tag'] = df_features['description'].apply(num_apperances_of_tag, args=('span',))\n",
    "df_features['a_tag'] = df_features['description'].apply(num_apperances_of_tag, args=('a',))\n",
    "df_features['u_tag'] = df_features['description'].apply(num_apperances_of_tag, args=('u',))\n",
    "\n",
    "#\n",
    "df_features['costsRating_quote'] = df_features['costsRating'] / 5\n",
    "df_features['executionTimeRating_quote'] = df_features['executionTimeRating'] / 5\n",
    "df_features['isPrepairationNeeded_quote'] = df_features['isPrepairationNeeded'] / 5\n",
    "\n",
    "# header image\n",
    "df_features['header_img'] = pd.notna(df_features['imageLink'])\n",
    "\n",
    "# text features\n",
    "df_features['text'] = df_features['description'].apply(get_text)\n",
    "df_features['count_question_mark'] = df_features['text'].apply(count_char, args=('?',)) / 20\n",
    "df_features['title_len'] = df_features['title'].str.len() / 40\n",
    "df_features['text_len'] = df_features['text'].str.len() / 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts_syntc = {\n",
    "    \"ADJ\": \"Adjektiv\",\n",
    "    \"ADP\": \"Adposition\",\n",
    "    \"ADV\": \"Adverb\",\n",
    "    \"AUX\": \"Hilfsverb\",\n",
    "    \"CONJ\": \"Koordinierende Konjunktionen\",\n",
    "    \"DET\": \"Artikel\",\n",
    "    \"INTJ\": \"Ausruf\",\n",
    "    \"NOUN\": \"Nomen\",\n",
    "    \"NUM\": \"Numerisch\",\n",
    "    \"PART\": \"particle\",\n",
    "    \"PRON\": \"Pronomen\",\n",
    "    \"PROPN\": \"Eigenname\",\n",
    "    \"PUNCT\": \"Satzzeichen\",\n",
    "    \"SCONJ\": \"unterordnende Konjunktion\",\n",
    "    \"SYM\": \"Symbol\",\n",
    "    \"VERB\": \"Verb\",\n",
    "    \"CCONJ\": \"Konjunktion\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_features\n",
    "\n",
    "tokens = []\n",
    "pos = []\n",
    "\n",
    "for doc in nlp.pipe(df_features['text'].astype('unicode').values, batch_size=50):\n",
    "    if doc.is_parsed:\n",
    "        pos.append([n.pos_ for n in doc])\n",
    "    else:\n",
    "        # We want to make sure that the lists of parsed results have the\n",
    "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "        pos.append(None)\n",
    "\n",
    "df_temp['species_pos'] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp['total_words'] = df_temp['species_pos'].apply(lambda x: len(x))\n",
    "df_temp['total_nomen'] = df_temp['species_pos'].apply(lambda L: len([x for x in L if 'NOUN' in x]))\n",
    "df_temp['total_adjektive'] = df_temp['species_pos'].apply(lambda L: len([x for x in L if 'ADJ' in x]))\n",
    "df_temp['total_numerisch'] = df_temp['species_pos'].apply(lambda L: len([x for x in L if 'NUM' in x]))\n",
    "df_temp['total_satzzeichen'] = df_temp['species_pos'].apply(lambda L: len([x for x in L if 'PUNCT' in x]))\n",
    "df_temp['total_konjunktion'] = df_temp['species_pos'].apply(lambda L: len([x for x in L if 'CCONJ' in x]))\n",
    "\n",
    "df_features['nomen_quote'] = df_temp['total_nomen'] / df_temp['total_words']\n",
    "df_features['adjektive_quote'] = df_temp['total_adjektive'] / df_temp['total_words']\n",
    "df_features['numerisch_quote'] = df_temp['total_numerisch'] / df_temp['total_words']\n",
    "df_features['satzzeichen_quote'] = df_temp['total_satzzeichen'] / df_temp['total_words']\n",
    "df_features['konjunktion_quote'] = df_temp['total_konjunktion'] / df_temp['total_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tag_name = df_tag[['id', 'name']].set_index('id').to_dict()['name']\n",
    "\n",
    "df_event_tags_short = df_event_tags[['event_id', 'tag_id']]\n",
    "df_event_tags_pivot = df_event_tags_short.pivot(index=\"event_id\", columns=\"tag_id\", values=\"tag_id\")\n",
    "df_event_tags_pivot = df_event_tags_pivot.fillna(False)\n",
    "df_event_tags_pivot[df_event_tags_pivot.columns] = df_event_tags_pivot[df_event_tags_pivot.columns].astype(bool)\n",
    "df_event_tags_features = df_event_tags_pivot.rename(columns=dict_tag_name)\n",
    "df_event_tags_features = df_event_tags_features.reset_index()\n",
    "\n",
    "list_tag_features = df_event_tags_features.columns.values.tolist()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Schnitzen',\n 'Backen',\n 'Unsere Erde',\n 'Pfa. Geschichte',\n 'Unser Bund',\n '1. Hilfe',\n 'Feuer machen',\n 'Versprechen',\n 'Karte Kompass',\n 'Kim-Spiele',\n 'Symbolik',\n 'Knoten',\n 'Küche',\n 'Schwarzzelte',\n 'Musisches',\n 'Haik',\n 'Baum',\n 'Sternenkunde',\n 'Handwerk',\n 'Spiele',\n 'Nachhaltigkeit',\n 'Wasser',\n 'Basteln',\n 'Geschichten',\n 'Unsere Sippe',\n 'Pflanzen',\n 'Forschen',\n 'Bewegung',\n 'Kreatives',\n 'Sommer',\n 'Herbst',\n 'Frühling',\n 'Winter',\n 'Im Haus',\n 'Garten',\n 'Videokonferenz',\n 'Alleine',\n 'Wölflinge',\n 'Pfadfinder',\n 'Rover',\n 'Lernen',\n 'Mit Abstand',\n 'Speziell zu Ostern',\n 'Speziell im Advent',\n 'Speziell zu Karneval',\n 'Wald',\n 'Ausflug',\n 'Gesellschaftliches']"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "list_tag_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_joined = pd.merge(df_features, df_event_tags_features, left_on=['id'], right_on= ['event_id'], how = 'left')\n",
    "\n",
    "\n",
    "\n",
    "df_features_joined = df_features_joined.drop(columns=['description', 'imageLink', 'createdBy', 'species_pos', 'event_id'])\n",
    "\n",
    "df_features_joined_num = df_features_joined.drop(columns=['title', 'created_at', 'text', 'total_words', 'total_nomen', 'total_adjektive', 'total_numerisch', 'total_satzzeichen', 'total_konjunktion', 'p_tag', 'img_tag', 'b_tag', 'li_tag', 'span_tag', 'a_tag', 'u_tag', 'costsRating', 'executionTimeRating', 'isPrepairationNeeded'])\n",
    "df_features_num = df_features_joined_num.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_joined.to_parquet('data/feature_vector.parquet', coerce_timestamps=\"us\")\n",
    "\n",
    "df_features_num.to_parquet('data/feature_vector_num.parquet', coerce_timestamps=\"us\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}